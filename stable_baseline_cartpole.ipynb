{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108986d3-1fdf-4c91-8e32-1fee468e5273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stable_baselines3.__version__='2.5.0'\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "\n",
    "print(f\"{stable_baselines3.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d4fba14-29f5-49a7-aa3c-c9b01c35f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym.__version__='1.0.0'\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "print(f\"{gym.__version__=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072c13ff-478b-490e-9112-545607324212",
   "metadata": {},
   "source": [
    "### RL algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a046c29-cc3d-4a8c-9886-95827d63ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea056ef9-e0a1-4076-80a6-177ead5f0855",
   "metadata": {},
   "source": [
    "The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n",
    "This step is optional as you can directly use strings in the constructor: \n",
    "\n",
    "```PPO('MlpPolicy', env)``` instead of ```PPO(MlpPolicy, env)```\n",
    "\n",
    "Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommened option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d50721a-f149-4e24-85c8-35cdb138105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.ppo import MlpPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042fe37-3651-48eb-bcc9-42b7258110ab",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb7d76a4-c915-4d7e-9b2e-4a861e4b12a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "model = PPO(MlpPolicy, env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "228fe173-ca8b-42bb-aec4-1e3b27b17b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In fact, Stable-Baselines3 already provides you with that helper:\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "965ef49b-862c-4b95-833e-302eb10d7cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:9.14 +/- 0.72\n"
     ]
    }
   ],
   "source": [
    "# Use a separate environement for evaluation\n",
    "eval_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c4dbd-ad52-4ca1-a0d4-7d86a12a5ae2",
   "metadata": {},
   "source": [
    "## Train the agent and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6341c9d6-048a-406b-8d69-14555be96609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x321f2e790>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the agent for 10000 steps\n",
    "model.learn(total_timesteps=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb0d463c-6615-4e47-9448-827112e41896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:449.37 +/- 77.40\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac24bd-a637-4492-b5ea-d81a64529eef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
